#########
# Basics
#########
- data flow: source systems => target systems
- need to think about: protocol (TCP, HTTP, REST), data format (binary, csv, json)

- source systems => Apache Kafka (high-throughput distributed messaging system) => target systems

- can scale to 100 brokers (servers)
- can scale to millions of messages per second
- latency = <10ms real-time

- Netflix uses Kafka to apply recommendations in real-time while watching TV shows
- Uber uses Kafka to gather user, taxi and trip data in real-time to compute surge pricing and forecast demand
- LinkedIn uses Kafka to prevent spam, collect user interactions for better recommendations


###############
# Kafka Theory
###############

# Topics
- a particular stream of data
- similar to table in a database
- can have many topics
- defined by its name

- topics are split in partitions
- each partition is ordered
- each message within a partition gets an incremental id, called offset

                             offsets
                             -------
               Partition 0 - 01234567...
Kafka Topic -  Partition 1 - 0123456789...
               Partition 2 - 012345...

- example
lots of trucks, each truck reports its GPS position to Kafka
can have a topic: trucks_gps that contains the position of all trucks
each truck will send a message to Kafka every 20 seconds
each message will contain the truck ID and the truck position (latitude and longitude)
we created topic with 10 partitions
consumer of kafka may be a location dashboard or a notification service

- Offset only have a meaning for a specific partition
- example
offset 3 in partition 0 doesn't represent the same data as offset 3 in partition 1

- order is guaranteed only within a partition - not across partitions
- data is deleted after a limited time (default 1 week)
- once the data is written to a partition, it can't be changed (immutability)
- data is assigned randomly to a partition unless a key is provided


# Brokers
- a Kafka cluster is composed of multiple brokers (servers)
- each broker is identified with its ID (integer)
- each broker contains certain topic partitions
- after connecting to any broker (called a bootstrap broker), will be connected to the entire cluster
- normal to start with 3 brokers but some big clusters have over 100 brokers

- example
3 brokers: 101, 102, 103

Topic-A with 3 partitions
101 = Topic-A, Partition 0
102 = Topic-A, Partition 2
103 = Topic-A, Partition 1

Topic-B with 2 partitions
101 = Topic-A, Partition 0; Topic-B, Partition 1
102 = Topic-A, Partition 2; Topic-B, Partition 0
103 = Topic-A, Partition 1


# Topic replication factor
- topics should have a replication factor > 1 (between 2 and 3)
- failover handling of broker
- only 1 broker can be a leader for a partition
- only leader can receive and serve data for that partition
- other brokers will synchronize the data
- thus each partition has 1 leader and multiple ISR (in-sync replica)

- example
3 brokers: 101, 102, 103

Topic-A with 2 partitions and replication factor of 2
101 = Topic-A, Partition 0 (leader)
102 = Topic-A, Partition 1 (leader); replicated Topic-A, Partition 0 from 101 (ISR)
103 = replicated Topic-A, Partition 1 from 102 (ISR)

if we lose 102, broker 101 and 103 can still serve the data


# Producers
- Producers write data to topics (which is made of partitions)
- Producers automatically know to which broker and partition to write to
- In case of Broker failures, Producers will automatically recover

- Producers can choose to receive ack of data writes:
acks=0: won't wait for ack
acks=1: will wait for leader ack
acks=all: Leader + replicas ack

- Producers can choose to send a key with the message (string, number, etc)
if key=null, data is sent round robin (broker 101, then 102, then 103...)
if a key is sent, then all messages for that key will always go to the same partition


# Consumers
- Consumers read data from a topic (identified by name)
- Consumers know which broker to read from
- In case of Broker failures, Consumers will automatically recover
- Data is read in order within each partitions

- Consumers read data in consumer groups
- Each consumer within a group reads from exclusive partitions
- If have more consumers than partitions, some consumers will be inactive

# Consumer offsets
- Kafka stores the offsets at which a consumer group has been reading
- these offsets are committed live in a Kafka topic named __consumer_offsets
- when a consumer in a group has processed data received from Kafka, it should be committing the offsets
- if a consumer dies, it will be able to read back from where it left off - thanks to the committed offsets

- Consumers choose when to commit offsets
- there are 3 delivery semantics:
at most once
 - offsets are committed as soon as the message is received
 - if the processing goes wrong, the message will be lost and it won't be read again
at least once (preferred)
 - offsets are committed after the message is processed
 - if the processing goes wrong, the message will be read again
 - this can result in duplicate processing of messages => need to ensure processing is idempotent
exactly once
 - can be achieved for Kafka => Kafka workflows using Kafka Streams API
 - for Kafka => External system workflows, need to use an idempotent consumer


# Kafka Broker Discovery
- every Kafka broker is also called a bootstrap server
- this means we only need to connect to one broker, and will be connected to the entire cluster
- each broker knows about all other brokers, topics and partitions (metadata)

Kafka Client -> 1. Connection + Metadata request -> Kafka cluster (Broker 101, 102, 103....)
             <- 2. List of all brokers
             -> 3. Can connect to the needed brokers


# Zookeeper
- manages brokers (keeps a list of them)
- helps in performing leader election for partitions
- sends notifications to Kafka in case of changes (e.g. new topic, broker dies, broker comes up, delete topics, etc...)
- Kafka cannot work without Zookeeper
- by design operates with an odd number of servers (3, 5, 7)
- has a leader (handle writes), the rest of the servers are followers (handle reads)
- does NOT store consumer offsets


# Kafka guarantees
- messages are appended to a topic-partition in the order they are sent
- consumers read messages in the order stored in a topic-partition
- with a replication factor of N, producers and consumers can tolerate up-to N-1 brokers being down
- as long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to the same partition


# Start Kafka
- first start zookeeper 
- change the data dir location in config/zookeeper.properties
- then start
zookeeper-server-start config/zookeeper.properties

- similarly change data dir location in config/server.properties
- then start kafka
kafka-server-start config/server.properties


###############################
## MAC-OS installation summary
###############################
In summary, for Mac OS X

- Install brew if needed: 
/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"

- Download and Setup Java 8 JDK:
brew tap caskroom/versions
brew cask install java8

- Download & Extract the Kafka binaries from https://kafka.apache.org/downloads

- Install Kafka commands using brew: 
brew install kafka

- Try Kafka commands using kafka-topics (for example)

- Edit Zookeeper & Kafka configs using a text editor
zookeeper.properties: dataDir=/your/path/to/data/zookeeper
server.properties: log.dirs=/your/path/to/data/kafka

- Start Zookeeper in one terminal window: 
zookeeper-server-start config/zookeeper.properties

- Start Kafka in another terminal window: 
kafka-server-start config/server.properties

################



############
# Kafka CLI
############


## Kafka Topics

kafka-topics main arguments:
 
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect
  connect to>                              to.

--topic <String: topic>                  The topic to create, alter, describe
                                           or delete. It also accepts a regular
                                           expression, except for --create
                                           option. Put topic name in double
                                           quotes and use the '\' prefix to
                                           escape regular expression symbols; e.
                                           g. "test\.topic".

--create                                 Create a new topic.

--partitions <Integer: # of partitions>  The number of partitions for the topic
                                           being created or altered (WARNING:
                                           If partitions are increased for a
                                           topic that has a key, the partition
                                           logic or ordering of the messages
                                           will be affected). If not supplied
                                           for create, defaults to the cluster
                                           default.

--replication-factor <Integer:           The replication factor for each
  replication factor>                      partition in the topic being
                                           created. If not supplied, defaults
                                           to the cluster default.

--list                                   List all available topics.

--describe                               List details for the given topics.

--delete                                 Delete a topic.


- final command connect to a Kafka server and create a topic
kafka-topics --bootstrap-server localhost:9092 --topic first_topic --create --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --topic second_topic --create --partitions 6 --replication-factor 1

- list all topics
kafka-topics --bootstrap-server localhost:9092 --list

- describe a topic
kafka-topics --bootstrap-server localhost:9092 --topic first_topic --describe

- delete a topic
kafka-topics --bootstrap-server localhost:9092 --topic second_topic --delete



## Kafka console producer

kafka-console-producer main arguments:

--bootstrap-server <String: server to    REQUIRED unless --broker-list
  connect to>                              (deprecated) is specified. The server
                                           (s) to connect to. The broker list
                                           string in the form HOST1:PORT1,HOST2:
                                           PORT2.

--topic <String: topic>                  REQUIRED: The topic id to produce
                                           messages to.

--producer-property <String:             A mechanism to pass user-defined
  producer_prop>                           properties in the form key=value to
                                           the producer.


- command to produce message to a broker
kafka-console-producer --bootstrap-server localhost:9092 --topic first_topic

- set the acks property
kafka-console-producer --bootstrap-server localhost:9092 --topic first_topic --producer-property acks=all

- produce to a new topic
kafka-console-producer --bootstrap-server localhost:9092 --topic new_topic_2



## Kafka console consumer

kafka-console-consumer main arguments:

--bootstrap-server <String: server to    REQUIRED: The server(s) to connect to.
  connect to>

--topic <String: topic>                  The topic to consume on.

--group <String: consumer group id>      The consumer group id of the consumer.


- command to consume messages from a topic (only new messages produced by producer will be consumed)
kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic

- command to consume all messages from a topic before the consumer was started
kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --from-beginning

- consumer group
kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-first-application

- second group
kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-second-application

- from beginning - only once all the messages are consumed and then the offset will be at the end of all the messages consumed
kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-second-application --from-beginning







